---
title: "Country Music Project"
author: "Matt Shu"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prereqs

```{r, message=FALSE, results = 'hide'}
# Needed to overcome error found below with Homebrew TBB vs bundled TBB: 
# https://github.com/RcppCore/RcppParallel/issues/182
# remotes::install_github("RcppCore/RcppParallel") 
library(igraph)
library(tidyverse)
library(stm)
library(RSQLite)
library(RecordLinkage)
library(stringdist)
library(devtools)
library(tm)
devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
library(tidystm)
```


## Preprocessing
Note: 
```{r}
conn <- dbConnect(RSQLite::SQLite(), "files/22-04-21-playback-fm-top-country.db")
dfSongs <- dbGetQuery(conn, 'SELECT * FROM lyrics')
dfArtists <- dbGetQuery(conn, 'SELECT * FROM artists')
dbDisconnect(conn)
```

## Dataset Visualizations
There are some duplicate songs for those who charted multiple years. Remove those songs
```{r}
dim(dfSongs)
dfSongs <- distinct(dfSongs, artist, track, .keep_all = TRUE)
dim(dfSongs)
```
Turn "nan" into NA's and select only variables of interest
```{r}
dfArtists[dfArtists == "nan"] <- NA
dfArtistsInterest <- dfArtists %>%
   dplyr::select(artist_id, mb_id, type, area.name, gender, life_span.begin, life_span.ended) %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r}
dfSongsArtists <- merge(dfSongs,dfArtistsInterest,by="artist_id")
```

```{r Remove Missing}
cleaned_df <-dfSongsArtists %>%
  # first, remove observation with missing values of the meta variables
  filter(!is.na(lyrics))  %>%
  # first, remove observation with missing values of the meta variables
  filter(!is.na(artist))  %>%
  as.data.frame()
cleaned_df$lyrics <- str_replace_all(cleaned_df$lyrics,"[\\s]+", " ")
```
## Create ID Hashes
```{r}
# cleaned_df$artist_id <- as.character(as.numeric(as.factor(cleaned_df$artist)))
cleaned_df$song_id <- as.character((10000 + as.numeric(as.factor(cleaned_df$track))))
```

## Filter out Mismatches

```{r Remove Erroneous Lyrics}
dim(cleaned_df)
cleaned_df$cleaned_lyrics <- 
  str_replace_all(cleaned_df$lyrics, 'Chap\\. [0-9]', NA_character_) %>%
  str_replace_all(., 'Listening Log', NA_character_) %>%
  str_replace_all(., 'Favorite Songs Of', NA_character_) %>%
  str_replace_all(., 'Chapter [0-9]', NA_character_) %>%
  str_replace_all(., 'New Music ', NA_character_) %>%
  str_replace_all(., 'Nominees', NA_character_) %>%
  str_replace_all(., 'Best Songs of ', NA_character_) %>%
  str_replace_all(., "[0-9]+ U S", NA_character_) %>% # Court Cases
  str_replace_all(., "[0-9]+ U.S", NA_character_) %>% # Court Cases
  str_replace_all(.,"[ ]+", " ") %>%
  str_replace(., ".*Lyrics", "")
cleaned_df <- cleaned_df %>%
  filter(!is.na(cleaned_lyrics)) %>%
  filter(levenshteinSim(track, str_match(lyrics, "(.*)Lyrics")[,2]) > .5) %>% # There are some false positives, when there are other languages
  as.data.frame()
dim(cleaned_df)
cleaned_df$lyrics_alnum <- cleaned_df$cleaned_lyrics %>% str_replace_all(., "[^[:alnum:]]", " ")
```

## Preprocessing (and STM exploration)
```{r}
# Dataframe containing the text
docs_df <- cleaned_df %>%
   dplyr::select(track_id, lyrics_alnum) %>%
   # first, remove observation with missing values of the meta variables
   filter(!is.na(lyrics_alnum))  %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r}
# Dataframe containing (sample) documents' metadata of interest
meta_df <- cleaned_df %>%
   dplyr::select(track_id, rank, artist, track, year) %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r, results='hide'}
processed_docs_1 <- textProcessor(documents = docs_df$lyrics_alnum, 
                                  metadata = meta_df, 
                                  lowercase = TRUE, 
                                  removestopwords = TRUE, 
                                  removenumbers = TRUE, 
                                  removepunctuation = TRUE, 
                                  ucp = TRUE,
                                  stem = TRUE, 
                                  striphtml = TRUE, 
                                  wordLengths = c(3, Inf),
                                  language = "en")
```

```{r}
meta <- processed_docs_1$meta
vocab <- processed_docs_1$vocab
docs <- processed_docs_1$documents
keep <- !is.na(meta$artist) && !is.na(meta$rank)
meta <- meta[keep,]
docs <- docs[keep]
```

```{r, results='hide'}
prepped_data <- prepDocuments(docs, 
                             vocab, 
                             meta,
                             # the lower threshold value means that only words
                             # that appear more times than the value (in this 
                             # example the value = 3) will be retained; this is 
                             # another researcher decision
                             lower.thresh = 2)
```


Old code for removing unusual mismatch with no words despite past filters
```{r, results='hide'}
length(docs_df$lyrics_alnum) # original documents
length(prepped_data$meta$track_id) # off from the preceding count
dif <- setdiff(docs_df$track_id, # original vector of documents
               prepped_data$meta$track_id) # list of documents after prepDocuments
tmp <- docs_df
tmp2 <- tmp[!tmp$track_id %in% dif,]
tmp_doc <- tmp2 %>%
  select(track_id, lyrics_alnum)
length(tmp_doc$track_id)
length(prepped_data$meta$track_id)

# View the track ids that were removed for some reason (often other language)
tmp3 <- tmp[tmp$track_id %in% dif,]
tmp3
```


See Cleaned Sample!
```{r}
head(cleaned_df)
```

### Find K
```{r}
k_seq = seq(4, 15, 1)
```

```{r, eval = FALSE}
## You can "watch" the algorithm model topics in the console
searched = searchK(prepped_data$documents,
                   prepped_data$vocab,
                   K = k_seq,
                   data = prepped_data$meta, 
                   seed = 183654)
saveRDS(searched, file = "22-04-29-searchK.RData")
```

### Show K
```{r}
searched <- readRDS("22-04-29-searchK.RData")
# Get values from `searchK` output
semcoh <- unlist(searched$results$semcoh)
exclus <- unlist(searched$results$exclus)

# Max/min semantic cohesion
max_sc <- max(semcoh)
min_sc<-min(semcoh)

# Max/min exclusivity
max_ex<-max(exclus)
min_ex<-min(exclus)

# Min-max normalization is (value - min)/(max - min)
x_vals <- (semcoh-min_sc)/(max_sc-min_sc)
y_vals <- (exclus-min_ex)/(max_ex-min_ex)
# add semantic cohesion and exclusivity together weighted evenly
search_plot_df <- tibble(id = k_seq, 
                   semcoh = x_vals,
                   exclus = y_vals, 
                   combine = x_vals*0.5 + y_vals*0.5)
# Plot
ggplot(search_plot_df, mapping = aes(x = semcoh, y = exclus)) +
  xlim(0,1) +
  ylim(0,1) +
  ggplot2::annotate("segment", x = 0, xend = 1, y = 0, yend = 1, color = "blue") +
  geom_label(aes(label=id))
```

### Model Work
```{r, results='hide',}
# 6 topics seems to also work nice, with a strong "Country" category
num_topics <- 7 # Chosen after above search and some playing around
out_covariates_7 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics,
                         prevalence = ~ rank + year,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

```
```{r}
head(out_covariates_7$theta) # each row is each document
# To find each artists, link the songs to the artists and then take the average for each artists, for each artist average of the columns
head(prepped_data$meta) # same order between dataframes
track_topic_df <- cbind(prepped_data$meta, out_covariates_7$theta)
```

```{r}
terms = labelTopics(out_covariates_7, n = 10)
terms$prob # rows are topics; columns are most probable words (in order)
terms$frex # rows are topics; columns are most FREX words (in order)

```

```{r}
# Parameters modified from: https://milesdwilliams15.github.io/Better-Graphics-for-the-stm-Package-in-R/
par(bty="n",lwd=5)
plot(out_covariates_7,
     type = "summary",
     main = "Prevalence of topics")

docs_examples_covar <- findThoughts(out_covariates_7,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:num_topics))

for(topic_num in c(1:num_topics)) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}

# Topic 1: Heartbreak Songs
# Topic 2: Cross-Country (Country Rock/Pop)
# Topic 3: Traditionalist Country (Pardi, Hank Williams)
# Topic 4: Bro-Country
# Topic 5: Sex Jams
# Topic 6: Love songs
# Topic 7: Family
topic_labels <- c("Heartbreak", "Cross-Country", "(Neo)-Traditionalist", "Bro-Country", "Sex Jams", "Love Songs", "Family")
```

```{r, eval=FALSE, include=FALSE}
out_covariates_8 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics + 1,
                         prevalence = ~ rank + year,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

plot(out_covariates_8,
     type = "summary",
     main = "Prevalence of topics")
docs_examples_covar_8 <- findThoughts(out_covariates_8,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:(num_topics+1)))

for(topic_num in c(1:(num_topics+1))) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar_8$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}
```

```{r}
eff1 <- estimateEffect(formula = c(1:num_topics) ~ s(year),
                      # the line above matches the model specification we used
                      stmobj = out_covariates_7,
                      meta = prepped_data$meta,
                      uncertainty = "Global")

# plot.estimateEffect(eff1,
#      covariate = "year",
#      topics = c(1:num_topics),
#      model = out_covariates_7,
#      method = "continuous",
#      xlab = "Year",
#      ylim=c(0, .4),
#      xlim=c(1940, 2020),
#      main = "Effect of Year on Topic Proportion")
```
```{r}
effect <- lapply(c(0, 1), function(i) {
  extract.estimateEffect(eff1,
     covariate = "year",
     topics = c(1:num_topics),
     model = out_covariates_7,
     method = "continuous")
})
effect <- do.call("rbind", effect)
effect <- effect %>% mutate(label = recode(topic, "1"=topic_labels[1], "2" = topic_labels[2], "3" = topic_labels[3], "4" = topic_labels[4], "5" = topic_labels[5], "6" = topic_labels[6], "7" = topic_labels[7]))
## And, for example, plot it with ggplot2 and facet by topic instead.
library(ggplot2)

ggplot(effect, aes(x = covariate.value, y = estimate,
                   ymin = ci.lower, ymax = ci.upper)) +
  facet_wrap(~ label, nrow = 2) +
  geom_ribbon(alpha = .5) +
  geom_line() +
  labs(x = "Year",
       y = "Expected Topic Proportion") + 
  scale_x_continuous(breaks=c(1940, 1960, 1980, 2000, 2020), 
   labels=waiver(), 
   expand=c(0,0), lim=c(0,100))

```


```{r, eval=FALSE}
eff <- estimateEffect(formula = c(1:num_topics) ~ year,
                      # the line above matches the model specification we used
                      stmobj = out_covariates_7,
                      meta = prepped_data$meta,
                      uncertainty = "Global")

# Second, plot the results
plot(eff,
     covariate = "year",
     topics = c(1:num_topics),
     model = out_covariates_7,
     method = "continuous",
     xlab = "Year",
     main = "Effect of Year on Topic Proportion")
```

```{r}
plot(topicCorr(out_covariates_7), 
  vlabels = topic_labels, vertex.label.cex = 1.0)
```
Topics 3, 2, 4, 7 are all related. This is an interesting finding! This suggests that traditionalist country especially seems related to both country rock/pop songs
Topic 2?: Country Rock/Pop
Topic 3: Traditionalist Country
Topic 4: Bro-Country
Topic 7: Family

## More on Topic Models
### Questions/Interests
- How would I see where individual artists fell in terms of topics?
 - In general, seeing prevalence of certain
 - Would it be, taking the top x documents for different topics and counting from there?
### More to Do?
- Plot covariate interaction!
  - Particularly interested in tracking gender * year interactions!

## ConText
```{r}
library(quanteda)
library(conText)
```

```{r}
glove <- readRDS("/Users/mattshu/Code/Country/files/glove.rds")
transform <- readRDS("/Users/mattshu/Code/Country/files/khodakA.rds")
```

```{r}
df_gendered <- cleaned_df %>%
  filter(gender == "male" | gender == "female")
```



```{r}
artist_meta_df <- df_gendered %>%
   dplyr::select(track_id, rank, artist, track, year, gender) %>%
   # the objects need to be class "data frame" 
   as.data.frame()
par_corpus <- quanteda::corpus(df_gendered$cleaned_lyrics, docvars = artist_meta_df)
```

```{r}
print(par_corpus)
```
```{r}
# From Vignette, modified
# tokenize corpus removing unnecessary (i.e. semantically uninformative) elements
toks <- tokens(par_corpus, remove_punct=T, remove_symbols=T, remove_numbers=T, remove_separators=T)

# clean out stopwords and words with 2 or fewer characters
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove", min_nchar=3)

toks_morenostop <- tokens_select(toks_nostop, pattern = c("gonna", "gotta", "cuz", "somebody", "maybe", "wanna", "yeah", "everybody", "shoulda", "kinda", "coulda", "ooh"), selection = "remove")
# only use features that appear at least 3 times in the corpus (follow Week 8 lecture)
feats <- dfm(toks_morenostop, tolower=T, verbose = FALSE) %>% dfm_trim(min_termfreq = 3) %>% featnames()

# leave the pads so that non-adjacent words will not become adjacent
toks <- tokens_select(toks_nostop, feats, padding = TRUE)
```

```{r, message=FALSE, warning=FALSE, results='hide'}
searchConText <- function(term) {
  # # Build a tokenized corpus of contexts surrounding the target term 
  target_toks <- tokens_context(x = toks, pattern = term, window = 6L)
  # Build document-feature matrix (documents x context counts)
  target_dfm <- dfm(target_toks)
  # Construct document-embedding-matrix
  target_dem <- dem(x = target_dfm, pre_trained = glove, transform = TRUE,
    transform_matrix = transform, verbose = TRUE)
  target_wv_group <- dem_group(target_dem,
                            groups = target_dem@docvars$gender)
  return(target_wv_group)
}
```


```{r}
# # Build a tokenized corpus of contexts surrounding the target term 
  target_toks <- tokens_context(x = toks, pattern = "american", window = 6L)
  # Build document-feature matrix (documents x context counts)
  target_dfm <- dfm(target_toks)
  # Construct document-embedding-matrix
  target_dem <- dem(x = target_dfm, pre_trained = glove, transform = TRUE,
    transform_matrix = transform, verbose = TRUE)

```

```{r}
head(docvars(target_toks))
```

```{r}

```

Now, embed the *contexts* in a pre-trained GloVe embedding space, getting a document-embedding matrix (DFM), or the embeddings of the *context* words

```{r, message=FALSE, warning=FALSE, results='hide'}

```

```{r}
# Average embeddings for each group
target_wv_group <- dem_group(target_dem,
                            groups = target_dem@docvars$gender)
```

```{r}
lives_nns <- nns(target_wv_group, pre_trained = glove, N = 5,
                 candidates = target_wv_group@features, as_list = TRUE)
```

```{r}
# Results for nationalists
lives_nns[["female"]]

# Results for Christians
lives_nns[["male"]]
```
```{r}
target_sim <- cos_sim(target_wv_group, pre_trained = glove,
                     features = c("love", "freedom"), as_list = TRUE)
```

```{r}
target_sim[["love"]]
target_sim[["freedom"]]
```

```{r}
flag_group <- searchConText("kill")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```
```{r}
flag_group <- searchConText("hell")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```
## Useful!
```{r}
flag_group <- searchConText("red white blue")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```

```{r}
flag_group <- searchConText("whiskey")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```

```{r}
flag_group <- searchConText("hometown")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```

```{r}
flag_group <- searchConText("man")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```


```{r}
flag_group <- searchConText("woman")
flag_nns <- nns(flag_group, pre_trained = glove, N = 5,
                 candidates = flag_group@features, as_list = TRUE)
# Results for nationalists
flag_nns[["female"]]

# Results for Christians
flag_nns[["male"]]
```

## Questions
- Can't get cr_glove to work?
- WHere did lives_dem come from?s
