---
title: "Country Music Project"
author: "Matt Shu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prereqs

```{r, message=FALSE, results = 'hide'}
library(igraph)
library(tidyverse)
library(stm)
library(RSQLite)
library(RecordLinkage)
library(stringdist)
```


## Preprocessing
```{r}
conn <- dbConnect(RSQLite::SQLite(), "files/22-04-21-playback-fm-top-country.db")
dfSongs <- dbGetQuery(conn, 'SELECT * FROM lyrics')
dfArtists <- dbGetQuery(conn, 'SELECT * FROM artists')
dbDisconnect(conn)
```

## Dataset Visualizations
```{r Remove Missing}
cleaned_df <-dfSongs %>%
  # first, remove observation with missing values of the meta variables
  filter(!is.na(lyrics))  %>%
  # first, remove observation with missing values of the meta variables
  filter(!is.na(artist))  %>%
  as.data.frame()
cleaned_df$lyrics <- str_replace_all(cleaned_df$lyrics,"[\\s]+", " ")
```
## Create Artist ID Hash
```{r}
cleaned_df$artist_id <- as.character(as.numeric(as.factor(cleaned_df$artist)))
cleaned_df$song_id <- as.character((10000 + as.numeric(as.factor(cleaned_df$track))))
```

## Filter out Mismatches

```{r Remove Erroneous Lyrics}
dim(cleaned_df)
cleaned_df$cleaned_lyrics <- 
  str_replace_all(cleaned_df$lyrics, 'Chap\\. [0-9]', NA_character_) %>%
  str_replace_all(., 'Listening Log', NA_character_) %>%
  str_replace_all(., 'Favorite Songs Of', NA_character_) %>%
  str_replace_all(., 'Chapter [0-9]', NA_character_) %>%
  str_replace_all(., 'New Music ', NA_character_) %>%
  str_replace_all(., 'Nominees', NA_character_) %>%
  str_replace_all(., 'Best Songs of ', NA_character_) %>%
  str_replace_all(., "[0-9]+ U S", NA_character_) %>% # Court Cases
  str_replace_all(., "[0-9]+ U.S", NA_character_) %>% # Court Cases
  # keep only alphabet letters and numbers ("al" and "num")
  str_replace_all(., "[^[:alnum:]]", " ") %>%
  # make multiple spaces into one space
  str_replace_all(.,"[ ]+", " ") %>%
  str_replace(., ".*Lyrics", "")
cleaned_df <- cleaned_df %>%
  filter(!is.na(cleaned_lyrics)) %>%
  filter(levenshteinSim(track, str_match(lyrics, "(.*)Lyrics")[,2]) > .5) %>% # There are some false positives, when there are other languages
  as.data.frame()
dim(cleaned_df)
```

## Preprocessing (and STM exploration)
```{r}
# Dataframe containing the text
docs_df <- cleaned_df %>%
   dplyr::select(track_id, cleaned_lyrics) %>%
   # first, remove observation with missing values of the meta variables
   filter(!is.na(cleaned_lyrics))  %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r}
# Dataframe containing (sample) documents' metadata of interest
meta_df <- cleaned_df %>%
   dplyr::select(track_id, rank, artist, track, year) %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r, results='hide'}
processed_docs_1 <- textProcessor(documents = docs_df$cleaned_lyrics, 
                                  metadata = meta_df, 
                                  lowercase = TRUE, 
                                  removestopwords = TRUE, 
                                  removenumbers = TRUE, 
                                  removepunctuation = TRUE, 
                                  ucp = TRUE,
                                  stem = TRUE, 
                                  striphtml = TRUE, 
                                  wordLengths = c(3, Inf),
                                  language = "en")
```

```{r}
meta <- processed_docs_1$meta
vocab <- processed_docs_1$vocab
docs <- processed_docs_1$documents
keep <- !is.na(meta$artist) && !is.na(meta$rank)
meta <- meta[keep,]
docs <- docs[keep]
```

```{r, results='hide'}
prepped_data <- prepDocuments(docs, 
                             vocab, 
                             meta,
                             # the lower threshold value means that only words
                             # that appear more times than the value (in this 
                             # example the value = 3) will be retained; this is 
                             # another researcher decision
                             lower.thresh = 2)
```


Old code for removing unusual mismatch with no words despite past filters
```{r, results='hide'}
length(docs_df$cleaned_lyrics) # original documents
length(prepped_data$meta$track_id) # off from the preceding count
dif <- setdiff(docs_df$track_id, # original vector of documents
               prepped_data$meta$track_id) # list of documents after prepDocuments
tmp <- docs_df
tmp2 <- tmp[!tmp$track_id %in% dif,]
tmp_doc <- tmp2 %>%
  select(track_id, cleaned_lyrics)
length(tmp_doc$track_id)
length(prepped_data$meta$track_id)

# View the track ids that were removed for some reason (often other language)
tmp3 <- tmp[tmp$track_id %in% dif,]
tmp3
```


See Cleaned Sample!
```{r}
head(cleaned_df)
```

### Find K
```{r}
k_seq = seq(4, 15, 1)
```

```{r, eval = FALSE}
## You can "watch" the algorithm model topics in the console
searched = searchK(prepped_data$documents,
                   prepped_data$vocab,
                   K = k_seq,
                   data = prepped_data$meta, 
                   seed = 183654)
# saveRDS(searched, file = "22-04-22-searchK-4-15.RData")
```

### Show K
```{r}
searched <- readRDS("22-04-22-searchK-4-15.RData")
# Get values from `searchK` output
semcoh <- unlist(searched$results$semcoh)
exclus <- unlist(searched$results$exclus)

# Max/min semantic cohesion
max_sc <- max(semcoh)
min_sc<-min(semcoh)

# Max/min exclusivity
max_ex<-max(exclus)
min_ex<-min(exclus)

# Min-max normalization is (value - min)/(max - min)
x_vals <- (semcoh-min_sc)/(max_sc-min_sc)
y_vals <- (exclus-min_ex)/(max_ex-min_ex)
# add semantic cohesion and exclusivity together weighted evenly
ids = k_seq
search_plot_df <- tibble(id = ids, 
                   semcoh = x_vals,
                   exclus = y_vals, 
                   combine = x_vals*0.5 + y_vals*0.5)

# Plot
ggplot(search_plot_df, mapping = aes(x = semcoh, y = exclus)) +
  xlim(0,1) +
  ylim(0,1) +
  annotate("segment", x = 0, xend = 1, y = 0, yend = 1, color = "blue") +
  geom_label(aes(label=id))
```

### Model Work
```{r, results='hide',}
# 6 topics seems to also work nice, with a strong "Country" category
num_topics <- 7 # Chosen after above search and some playing around
out_covariates_7 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics,
                         prevalence = ~ rank * year,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

```
```{r}
terms = labelTopics(out_covariates_7, n = 10)
terms$prob # rows are topics; columns are most probable words (in order)
terms$frex # rows are topics; columns are most FREX words (in order)

```

```{r}
# Parameters modified from: https://milesdwilliams15.github.io/Better-Graphics-for-the-stm-Package-in-R/
par(bty="n",lwd=5)
plot(out_covariates_7,
     type = "summary",
     main = "Prevalence of topics")

docs_examples_covar <- findThoughts(out_covariates_7,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:num_topics))

for(topic_num in c(1:num_topics)) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}

# Topic 1: Heartbreak Songs
# Topic 2: Cross-Country (Country Rock/Pop)
# Topic 3: Traditionalist Country (Pardi, Hank Williams)
# Topic 4: Bro-Country
# Topic 5: Sex Jams
# Topic 6: Love songs
# Topic 7: Family
topic_labels <- c("Heartbreak", "Cross-Country", "Traditionalist", "Bro-Country", "Sex Jams", "Love Songs", "Family")
```

```{r, eval=FALSE, include=FALSE}
out_covariates_8 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics + 1,
                         prevalence = ~ rank + year,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

plot(out_covariates_8,
     type = "summary",
     main = "Prevalence of topics")
docs_examples_covar_8 <- findThoughts(out_covariates_8,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:(num_topics+1)))

for(topic_num in c(1:(num_topics+1))) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar_8$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}
```

```{r}
eff <- estimateEffect(formula = c(1:num_topics) ~ year,
                      # the line above matches the model specification we used
                      stmobj = out_covariates_7,
                      meta = prepped_data$meta,
                      uncertainty = "Global")

# Second, plot the results
plot(eff,
     covariate = "year",
     topics = c(1:num_topics),
     model = out_covariates_7,
     method = "continuous",
     xlab = "Year",
     main = "Effect of Year on Topic Proportion")
```

```{r}
plot(topicCorr(out_covariates_7), 
  vlabels = topic_num, vertex.label.cex = 1.0)
```
Topics 3, 2, 4, 7 are all related. This is an interesting finding! This suggests that traditionalist country especially seems related to both country rock/pop songs
Topic 2?: Country Rock/Pop
Topic 3: Traditionalist Country
Topic 4: Bro-Country
Topic 7: Family

## More on Topic Models
### Questions/Interests
- How would I see where individual artists fell in terms of topics?
 - In general, seeing prevalence of certain
 - Would it be, taking the top x documents for different topics and counting from there?
### More to Do?
- Plot covariate interaction!
  - Particularly interested in tracking gender * year interactions!

## Word Embeddings - Quanteda
```{r}
library(quanteda)
library(conText)
```

