---
title: "Country Music Project"
author: "Matt Shu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prereqs

```{r, message=FALSE, results = 'hide'}
# Needed to overcome error found below with Homebrew TBB vs bundled TBB: 
# https://github.com/RcppCore/RcppParallel/issues/182
# remotes::install_github("RcppCore/RcppParallel") 
library(igraph)
library(tidyverse)
library(stm)
library(RSQLite)
library(RecordLinkage)
library(stringdist)
library(devtools)
library(tm)
devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
library(tidystm)
```

```{r}
conn <- dbConnect(RSQLite::SQLite(), "files/22-04-21-playback-fm-top-country.db")
cleaned_df <- dbGetQuery(conn, 'SELECT * FROM cleaned')
dbDisconnect(conn)
```
```{r}
names(cleaned_df)
```

## Preprocessing (and STM exploration)
```{r}
# Dataframe containing the text
docs_df <- cleaned_df %>%
   dplyr::select(track_id, lyrics_alnum) %>%
   # first, remove observation with missing values of the meta variables
   filter(!is.na(lyrics_alnum))  %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r}
# Dataframe containing (sample) documents' metadata of interest
meta_df <- cleaned_df %>%
   dplyr::select(track_id, rank, artist, track, year, gender) %>%
   # the objects need to be class "data frame" 
   as.data.frame()
```

```{r, results='hide'}
processed_docs_1 <- textProcessor(documents = docs_df$lyrics_alnum, 
                                  metadata = meta_df, 
                                  lowercase = TRUE, 
                                  removestopwords = TRUE, 
                                  removenumbers = TRUE, 
                                  removepunctuation = TRUE, 
                                  ucp = TRUE,
                                  stem = TRUE, 
                                  striphtml = TRUE, 
                                  wordLengths = c(3, Inf),
                                  language = "en")
```

```{r}
meta <- processed_docs_1$meta
vocab <- processed_docs_1$vocab
docs <- processed_docs_1$documents
keep <- !is.na(meta$artist) && !is.na(meta$rank)
meta <- meta[keep,]
docs <- docs[keep]
```

```{r, results='hide'}
prepped_data <- prepDocuments(docs, 
                             vocab, 
                             meta,
                             # the lower threshold value means that only words
                             # that appear more times than the value (in this 
                             # example the value = 3) will be retained; this is 
                             # another researcher decision
                             lower.thresh = 2)
```


Old code for removing unusual mismatch with no words despite past filters
```{r, results='hide'}
length(docs_df$lyrics_alnum) # original documents
length(prepped_data$meta$track_id) # off from the preceding count
dif <- setdiff(docs_df$track_id, # original vector of documents
               prepped_data$meta$track_id) # list of documents after prepDocuments
tmp <- docs_df
tmp2 <- tmp[!tmp$track_id %in% dif,]
tmp_doc <- tmp2 %>%
  select(track_id, lyrics_alnum)
length(tmp_doc$track_id)
length(prepped_data$meta$track_id)

# View the track ids that were removed for some reason (often other language)
tmp3 <- tmp[tmp$track_id %in% dif,]
tmp3
```


See Cleaned Sample!
```{r}
head(cleaned_df)
```

### Find K
```{r}
k_seq = seq(4, 15, 1)
```

```{r, eval = FALSE}
## You can "watch" the algorithm model topics in the console
searched = searchK(prepped_data$documents,
                   prepped_data$vocab,
                   K = k_seq,
                   data = prepped_data$meta, 
                   seed = 183654)
saveRDS(searched, file = "files/22-04-29-searchK.RData")
```

### Show K
```{r, cache=TRUE}
searched <- readRDS("files/22-04-29-searchK.RData")
# Get values from `searchK` output
semcoh <- unlist(searched$results$semcoh)
exclus <- unlist(searched$results$exclus)

# Max/min semantic cohesion
max_sc <- max(semcoh)
min_sc<-min(semcoh)

# Max/min exclusivity
max_ex<-max(exclus)
min_ex<-min(exclus)

# Min-max normalization is (value - min)/(max - min)
x_vals <- (semcoh-min_sc)/(max_sc-min_sc)
y_vals <- (exclus-min_ex)/(max_ex-min_ex)
# add semantic cohesion and exclusivity together weighted evenly
search_plot_df <- tibble(id = k_seq, 
                   semcoh = x_vals,
                   exclus = y_vals, 
                   combine = x_vals*0.5 + y_vals*0.5)
# Plot
ggplot(search_plot_df, mapping = aes(x = semcoh, y = exclus)) +
  xlim(0,1) +
  ylim(0,1) +
  ggplot2::annotate("segment", x = 0, xend = 1, y = 0, yend = 1, color = "blue") +
  geom_label(aes(label=id))
```

### Model Work
```{r, results='hide',}
# 6 topics seems to also work nice, with a strong "Country" category
num_topics <- 7 # Chosen after above search and some playing around
out_covariates_7 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics,
                         prevalence = ~ rank + year * gender,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

```
```{r}
head(out_covariates_7$theta) # each row is each document
# To find each artists, link the songs to the artists and then take the average for each artists, for each artist average of the columns
head(prepped_data$meta) # same order between dataframes
track_topic_df <- cbind(prepped_data$meta, out_covariates_7$theta)
```

```{r}
terms = labelTopics(out_covariates_7, n = 10)
terms$prob # rows are topics; columns are most probable words (in order)
terms$frex # rows are topics; columns are most FREX words (in order)

```

```{r}
# Parameters modified from: https://milesdwilliams15.github.io/Better-Graphics-for-the-stm-Package-in-R/
par(bty="n",lwd=5)
plot(out_covariates_7,
     type = "summary",
     main = "Prevalence of topics")

docs_examples_covar <- findThoughts(out_covariates_7,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:num_topics))

for(topic_num in c(1:num_topics)) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}

# Topic 1: Heartbreak Songs
# Topic 2: Cross-Country (Country Rock/Pop)
# Topic 3: Traditionalist Country (Pardi, Hank Williams)
# Topic 4: Bro-Country
# Topic 5: Sex Jams
# Topic 6: Love songs
# Topic 7: Family
topic_labels <- c("Heartbreak", "Cross-Country", "(Neo)-Traditionalist", "Bro-Country", "Sex Jams", "Love Songs", "Family")
```

```{r, eval=FALSE, include=FALSE}
out_covariates_8 <- stm(prepped_data$documents,
                         prepped_data$vocab,
                         K = num_topics + 1,
                         prevalence = ~ rank + year,
                         max.em.its = 500,
                         data = prepped_data$meta,
                         seed = 592669)

plot(out_covariates_8,
     type = "summary",
     main = "Prevalence of topics")
docs_examples_covar_8 <- findThoughts(out_covariates_8,
                               texts = tmp_doc$track_id,
                               n = 10,
                               topics = c(1:(num_topics+1)))

for(topic_num in c(1:(num_topics+1))) {
  print(paste("Topic ", topic_num))
  for(track in docs_examples_covar_8$docs[[topic_num]]) {
    print(cleaned_df$track[cleaned_df$track_id == track])
  }
  print("")
}
```

```{r}
eff1 <- estimateEffect(formula = c(1:num_topics) ~ s(year),
                      # the line above matches the model specification we used
                      stmobj = out_covariates_7,
                      meta = prepped_data$meta,
                      uncertainty = "Global")

# plot.estimateEffect(eff1,
#      covariate = "year",
#      topics = c(1:num_topics),
#      model = out_covariates_7,
#      method = "continuous",
#      xlab = "Year",
#      ylim=c(0, .4),
#      xlim=c(1940, 2020),
#      main = "Effect of Year on Topic Proportion")
```

```{r}
effect <- lapply(c(0, 1), function(i) {
  extract.estimateEffect(eff1,
     covariate = "year",
     topics = c(1:num_topics),
     model = out_covariates_7,
     method = "continuous")
})
effect <- do.call("rbind", effect)
effect <- effect %>% mutate(label = recode(topic, "1"=topic_labels[1], "2" = topic_labels[2], "3" = topic_labels[3], "4" = topic_labels[4], "5" = topic_labels[5], "6" = topic_labels[6], "7" = topic_labels[7]))
## And, for example, plot it with ggplot2 and facet by topic instead.
library(ggplot2)

ggplot(effect, aes(x = covariate.value, y = estimate,
                   ymin = ci.lower, ymax = ci.upper)) +
  facet_wrap(~ label, nrow = 2) +
  geom_ribbon(alpha = .5) +
  geom_line() +
  labs(x = "Year",
       y = "Expected Topic Proportion") + 
  scale_x_continuous(breaks=c(1940, 1960, 1980, 2000, 2020), 
   labels=waiver(), lim=c(1940,2020)) + 
  theme(panel.spacing = unit(1, "lines"))

```


```{r, eval=FALSE}
eff <- estimateEffect(formula = c(1:num_topics) ~ year,
                      # the line above matches the model specification we used
                      stmobj = out_covariates_7,
                      meta = prepped_data$meta,
                      uncertainty = "Global")

# Second, plot the results
plot(eff,
     covariate = "year",
     topics = c(1:num_topics),
     model = out_covariates_7,
     method = "continuous",
     xlab = "Year",
     main = "Effect of Year on Topic Proportion")
```

```{r}


library(huge)
topic_corr <- topicCorr(out_covariates_7, method = "huge")
topic_corr
set.seed(5)
plot(topic_corr, 
  vlabels = topic_labels, vertex.label.cex = 1, layout =  layout.auto)
```
Topics 3, 2, 4, 7 are all related. This is an interesting finding! This suggests that traditionalist country especially seems related to both country rock/pop songs
Topic 2?: Country Rock/Pop
Topic 3: Traditionalist Country
Topic 4: Bro-Country
Topic 7: Family

## More on Topic Models
### Questions/Interests
- How would I see where individual artists fell in terms of topics?
 - In general, seeing prevalence of certain
 - Would it be, taking the top x documents for different topics and counting from there?
### More to Do?
- Plot covariate interaction!
  - Particularly interested in tracking gender * year interactions!

